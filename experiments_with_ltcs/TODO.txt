
        WHAT                            |               STATUS


Organisational  
        decide on binning                       done. 50 ms 50% overlap (8 07)
        make neuron data class                  done.           
        inspect 17-neuron dataset               done. select 17 neuron, exclude neuron 9(8) (8 07)
        plot predictions and error              done.
        select during sound /laser activation   done. (12 07)
                extra class?
                concat pulses on dimension
                calcuate how many bins between activations
        gaussian kernel                         done.
        install gpu                             done. (10 07)
        troubleshoot gpu                        done. (2 days in week of 1 07)
        better train test split                 done. 12 07
        where test data is not part of          done. (12 07)
        look into left hamming somoothing       done. 16 07. use gaussian smoothing
        re-name the cv folders such that        queue(1)
                naming is correct


Model fitting
        train with / without noise              done. continue without noise (week of 1/07 or week before)
        experiment with mixed memory            done. use mixed memory
        use predicted bins loss function        done.       (8 07)
        test predictions on BEST Model          done. (12 07)
        compare iterative prediction with       pending 1807
                future 5 prediction             
        
        cross validation for sigma              running 31 07
        cross validation for sequence length    running 31 07
        implement cross validation              done. 31.07
        cross validation for model params       queue(2). re-run after decision on sigma and sequence length. now running with (7,32)　on ponyland


Model implementation
        predict more timestaps ahead            experimented with 2 timesteps  
        predict with laser activations  
                optuna                          done (13-15 7)
                runnning with both loss funcs   trial 31, trial 34, trial 45
        connect to kaede                        done  (10 07)
        multi-step forecasting plotting         done 16 07
                code share with non-sequential
                make prediction set
        sequential prediction set               done 16 07
                obtain test chunk       
                make test data with 1 increment for val score 
                >make predict data with increment = future
                >normalize predict data 
                >make predict dataloader
        incremental prediction plotting         done. 1707
        scheduled sampling in training          running 05 08. set sigma and model size and sequence length (7,32,48). tuning lr.
        revisit chunk selection.                done 16 07. training data seqs are from same chunk. sequential val and test chunks. random train chunks
        real-time implementation in python      done (24/25/26/29 07)
                first with threads and locks    
                now with multiprocessing and pipes
                data loads only new bins to decrease
                communication overhead
        read into jean`s work                   pending 16,17 07
        write code for resuming                 done 18 07
        implement predict datasets              done 2207
                for other classes, if necessary 
        implement realtime prediction           done. 2207 left to do : convert to bins
                and plotting   
        for plots include the laser activation  done. 2207 (not in plot but present in data)
                at the first future position 
                (-future)!
        implement predict function in learner   done 30 07 
        read into jean's system                 done 30 07 

        plot laser ticks with the results           done 06/08
        fix plot function + run for 400 epochs  done 2307
                + evaluate all models           
        fix dataloader for realtime predictions.        done 08/08. obtain normalized value in predict() function
                make sure that the laser activation     
                is the same as the normalized values
                        during training

        

finalise to do 
        HYPERPARAMETERS
                seq length : 0508
                model size and lr : 0608
                scheduled sampling : 0708
                weighted batch sampling? : running (0708 implemented)

        
        ANALYSIS
                Retrain with hyperparams on new dataset (queue 0, check if weighted batch sampling works good)
                Compare prediction error for each lag point
                Plot pos and neg laser for each timepoint 
                Plot error+sd with pos-neg laser difference + sd to see if we can reliably predict difference



obtain model parameters                         DONE: SELECT SIZE 36 LENGTH 48 LR 0.005 AND COMPARE SCHEDULED SAMPLING
        so far: seq len (48) and size (32)
        train 5-fold with seq len 48 and size and lr ?? ; (20,0.01; 22, 0.01; 24,0.01 )
        conclude that we need a lower lr of about 5e-4 for slow convergence
        go check speed diff between 40 and 32 size
        
train normal and finetune with scheduled sampling  SKIP     
        find out how to load only model weights
        call other learning module

compare training with and without scheduled sampling (running)　
        by using the same parameters
        write script for 5-fold cv with these techniques (takes 20 hours for scheduled sampling) done 06/08. now running       
                2 experiments per type: low and hig lr
        perform 5-fold cross validation: plot mean auc (or whatever)
over-sampling of laser timesteps?: implement weighted sampler with batch sampling ! 
        overample any sequence that has laser activity in the last 5 bins
present by evaluation metric: plot MAE over time ? together with pooled MAE
train final model on all training data  
        implement training without validation

today todo (1707)
       RERUN AL INFERENCE ON BEST EPOCH

        


running ALL 10-2
running FUTURE LOSS 10-0 

redo 1604 1603 best//last with 5 history   DONE

redo all above 1808 best

evaluate 1907 again
enter 1811 resulst

export the results as images
crop the images
paste into a new powerpoint
rerunning all models

TO DO :

handle the fact that the realtime data does not have laser data
and therefore this is not normalised

python3 forecast.py --epochs 50 --gpus 0 --initial_lr 0.001 --dataset neuronlaser  \
        --seq_len "optimise"  --mixed_memory --future 1 --future_loss --size 32 --sigma 7