FUTURE　| BIN_D |   inc |   MIXED MEM | LR       | SIZE |   TEST  | ID    | LOSS  | HPARAMS | LAST LOSS
1           0.5     2       0           9.7E-3      39      6.327   0409    ALL     0308     
1           0.5     2       0           6.45E-4     23      9.083   0410    ALL     0308 
1           0.5     2       1           7.28E-3     27      5.22    0504    ALL         
1           0.5     2       1           9.29E-4     39      4.31    0506    ALL
>　use mixed memory with large network SIZE
2           0.5     2       1           9.29E-4     39      8.39    0506    ALL                                   
2           0.05    53      1           6.62E-3     28      0.4704  1203    ALL     0910           	    
>　how is small binning?
2           0.05    53      1                          <LOSS ERROR> 1002    FUTURE  0911            
2           0.05    53      1           7.67E-4     20     0.8387   1103    FUTURE  1006            
2           0.05    53      1           9.74E-3     24     0.7850   1104    FUTURE  1006              
>  does other loss improve?       
4           0.05    53                  9.9E-3      40     0.7913   1110            1017               dcluser
> how is performance for predicting 0.05*4(0.2) sec? 
TIMESTAMP    
2           0.05    53      1           0.022       36     0.4774   1208    ALL     
> does including timestamp improve performance?
OTHER TRAINSPLIT METHOD
5           0.05    53      1	        9.51e-4	    24     0.7293   1603    FUTURE  1419 0.9451
5           0.05    53      1           2.96e-4 	32     0.7485   1604    FUTURE  1419 0.74855  < best not done training. 2304 for 400 epochs ( 0.9103 | 0.73073 )
5           0.05    53      1           1/2e-4      36     0.7302   2205    FUTURE　　　  0.7379 

5           0.05    53      1           1.57e-3     32     0.7209   1607    ALL     1512 0.8400
5           0.05    53      1           1.37e-4     32     0.7411   1608    ALL     1512 0.7411  < best not done training. 2303 for 400 epochs (0.7266 | 0.7244)
5           0.05    53      1           1/5e-4      36     0.7241   2206    ALL　        0.7243 
these　models tend to overfit
2304 vs 1604: 2304 last fits a bit better than  any 1604 but stil doesn`t fit to infrequent spikes
2303 vs 1608 : again. none of them really fit

1           0.05    53      1           0.005       36     0.3680   1808    FUTURE　　　 0.5038
1           0.05    53      1           0.001       32     0.6043   2302    FUTURE　　　 0.6043     for 400 epochs
1           0.05    53      1           0.001       32              3012    FUTURE　　              400 epochs. new binning  | store these in other checkpoint name like:  be sure to select the correct dataset!
python3 forecast.py --epochs 400 --gpus 2 --initial_lr 0.001 --dataset neuronlaser \
    --seq_len 32  --mixed_memory --future 5 --future_loss --size 32 --checkpoint_id 2024073012 --model_id 2024073014 --iterative_forecast 
1           0.05    53      1           0.001       32              3011    FUTURE　　              400 epochs. new binning. n-20 dataset
python3 forecast.py --epochs 400 --gpus 2 --initial_lr 0.001 --dataset neuronlaser \
    --seq_len 32  --mixed_memory --future 5 --future_loss --size 32 --checkpoint_id 2024073011 --model_id 2024073013 --iterative_forecast 
1           0.05    53      1           0.022       36     0.2314   1807    ALL　        0.2373
1           0.05    53      1           0.001       32     0.2433   2301    ALL　        0.2593     for 400 epochs
2302 vs 1808 whereas 2302 mamanges to fit the infrequent neuron, 1808 fits better to the other spikes
2301 vs 1807 : not much different. I would use 2301 as it converged slower 


>　how is performance with timestamp data ?
(for iterative prediction)[]
1/5         0.05    53      1                                              FUTURE   1708    used best val loss . change to last. redundant
1/5         0.05    53      1          	0.00181 	48      0.4054   1907  FUTURE   1711    0.5295 both overfit  < best
1/5         0.05    53      1           0.01        36      0.3449   1903  FUTURE     -     0.4625

1/5         0.05    53      1           0.01        48      0.6338   1810  ALL              0.2461
1/5         0.05    53      1           0.02        36      0.2342   1811  ALL              0.2348 < best
1/5         0.05    53      1           0.022       36      0.2330   2209                   0.2407         1807 settings

python forecast.py --size 36 --epochs 200 --gpus 1 --initial_lr 0.022 --dataset neuronlaser --future 2 --seq_len 32 --binwidth 0.05

Comparisons

new hyperparam tuning
1/5         0.05    53      1                                                       80104           sigma 7 seq len 32 10-0/1, 13-0, 3-0

FUTURE      LOSS        INCREMENTAL         ID
5           ALL         -                   2206
5           FUTURE      -                   2205
1           ALL         -                   1807
1           FUTURE      -                   1808
5           ALL         +                   1811
5           FUTURE      +                   1907

